{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9534ee63-e7b7-4a1d-81c8-a80787f67123",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71534423-86b1-4d67-b6f1-82884897ca76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__boundary__         computed  \n",
      "__characters__       computed  \n",
      "__levDown__          computed  \n",
      "__levUp__            computed  \n",
      "__levels__           computed  \n",
      "__order__            computed  \n",
      "__rank__             computed  \n",
      "__sections__         computed  \n",
      "__structure__        NOT LOADED\n",
      "book                 node (str)\n",
      "book@en              node (str)\n",
      "chapter              node (int)\n",
      "code                 NOT LOADED\n",
      "det                  node (str)\n",
      "dist                 NOT LOADED\n",
      "dist_unit            NOT LOADED\n",
      "distributional_parent NOT LOADED\n",
      "domain               NOT LOADED\n",
      "function             NOT LOADED\n",
      "functional_parent    NOT LOADED\n",
      "g_cons               node (str)\n",
      "g_cons_utf8          node (str)\n",
      "g_lex                node (str)\n",
      "g_lex_utf8           node (str)\n",
      "g_nme                node (str)\n",
      "g_nme_utf8           node (str)\n",
      "g_pfm                node (str)\n",
      "g_pfm_utf8           node (str)\n",
      "g_prs                NOT LOADED\n",
      "g_prs_utf8           NOT LOADED\n",
      "g_suffix             node (str)\n",
      "g_suffix_utf8        node (str)\n",
      "g_uvf                node (str)\n",
      "g_uvf_utf8           node (str)\n",
      "g_vbe                node (str)\n",
      "g_vbe_utf8           node (str)\n",
      "g_vbs                node (str)\n",
      "g_vbs_utf8           node (str)\n",
      "g_word               node (str)\n",
      "g_word_utf8          node (str)\n",
      "gn                   NOT LOADED\n",
      "is_root              NOT LOADED\n",
      "kind                 NOT LOADED\n",
      "label                NOT LOADED\n",
      "language             node (str)\n",
      "lex                  node (str)\n",
      "lex_utf8             node (str)\n",
      "lexeme_count         NOT LOADED\n",
      "ls                   NOT LOADED\n",
      "mother               NOT LOADED\n",
      "mother_object_type   NOT LOADED\n",
      "nme                  NOT LOADED\n",
      "nu                   node (str)\n",
      "number               NOT LOADED\n",
      "oslots               edge      \n",
      "otext                config    \n",
      "otype                node (str)\n",
      "pdp                  node (str)\n",
      "pfm                  NOT LOADED\n",
      "prs                  node (str)\n",
      "prs_gn               node (str)\n",
      "prs_nu               NOT LOADED\n",
      "prs_ps               NOT LOADED\n",
      "ps                   node (str)\n",
      "qere                 NOT LOADED\n",
      "qere_utf8            NOT LOADED\n",
      "rela                 NOT LOADED\n",
      "sp                   node (str)\n",
      "st                   NOT LOADED\n",
      "suffix_gender        NOT LOADED\n",
      "suffix_number        NOT LOADED\n",
      "suffix_person        NOT LOADED\n",
      "tab                  NOT LOADED\n",
      "txt                  NOT LOADED\n",
      "typ                  NOT LOADED\n",
      "uvf                  NOT LOADED\n",
      "vbe                  NOT LOADED\n",
      "vbs                  NOT LOADED\n",
      "verse                node (int)\n",
      "vs                   NOT LOADED\n",
      "vt                   NOT LOADED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Computed',\n",
       "  'computed-data',\n",
       "  ('C Computed', 'Call AllComputeds', 'Cs ComputedString')),\n",
       " ('Features', 'edge-features', ('E Edge', 'Eall AllEdges', 'Es EdgeString')),\n",
       " ('Fabric', 'loading', ('TF',)),\n",
       " ('Locality', 'locality', ('L Locality',)),\n",
       " ('Nodes', 'navigating-nodes', ('N Nodes',)),\n",
       " ('Features',\n",
       "  'node-features',\n",
       "  ('F Feature', 'Fall AllFeatures', 'Fs FeatureString')),\n",
       " ('Search', 'search', ('S Search',)),\n",
       " ('Text', 'text', ('T Text',))]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections, itertools, os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from tf.fabric import Fabric\n",
    "\n",
    "TF = Fabric(locations='~/github//extrabiblical/tf/0.2') \n",
    "\n",
    "api = TF.load('''\n",
    "        otype lex language prs pdp\n",
    "        det nu ps sp\n",
    "        prs prs_gn\n",
    "        g_suffix\n",
    "        g_cons_utf8\n",
    "        g_lex_utf8 g_nme_utf8 g_pfm_utf8 g_vbs_utf8 g_vbe_utf8 g_uvf_utf8\n",
    "        g_lex g_nme g_pfm g_vbs g_vbe g_uvf\n",
    "            ''')\n",
    "    \n",
    "api.loadLog()\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5984bea-81a0-4fb2-bae2-94b4f7fa1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './models'\n",
    "seq_length = 2\n",
    "augment_factor = 10\n",
    "\n",
    "# model details\n",
    "num_hidden_layers = 2\n",
    "num_attention_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ab9564-621b-484e-a58d-9059f3e6dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_chars_utf8 = {' ',\n",
    " 'א',\n",
    " 'ב',\n",
    " 'ג',\n",
    " 'ד',\n",
    " 'ה',\n",
    " 'ו',\n",
    " 'ז',\n",
    " 'ח',\n",
    " 'ט',\n",
    " 'י',\n",
    " 'ך',\n",
    " 'כ',\n",
    " 'ל',\n",
    " 'ם',\n",
    " 'מ',\n",
    " 'ן',\n",
    " 'נ',\n",
    " 'ס',\n",
    " 'ע',\n",
    " 'ף',\n",
    " 'פ',\n",
    " 'ץ',\n",
    " 'צ',\n",
    " 'ק',\n",
    " 'ר',\n",
    " 'ש',\n",
    " 'ת'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b661bb6-e68d-4d9c-84f0-ebf841e33882",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_dict_heb = {char: char for char in relevant_chars_utf8}\n",
    "double_chars = ['ןנ','ףפ', 'ץצ','ךכ','םמ']\n",
    "\n",
    "for end_char, non_end_char in double_chars:\n",
    "    alphabet_dict_heb[end_char] = non_end_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28f343aa-f9d8-4774-b33e-1a7430973028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for conversion of lex to hebrew script, including markers =, / and [\n",
    "\n",
    "alphabet_dict_heb_lat = {'א': '>',\n",
    "                                      'ב': 'B',\n",
    "                                      'ג': 'G',\n",
    "                                      'ד': 'D',\n",
    "                                      'ה': 'H',\n",
    "                                      'ו': 'W',\n",
    "                                      'ז': 'Z',\n",
    "                                      'ח': 'X',\n",
    "                                      'ט': 'V',\n",
    "                                      'י': 'J',\n",
    "                                      'כ': 'K',\n",
    "                                      'ל': 'L',\n",
    "                                      'מ': 'M',\n",
    "                                      'נ': 'N',\n",
    "                                      'ס': 'S',\n",
    "                                      'ע': '<',\n",
    "                                      'פ': 'P',\n",
    "                                      'צ': 'Y',\n",
    "                                      'ק': 'Q',\n",
    "                                      'ר': 'R',\n",
    "                                      'ש': 'C',\n",
    "                                      'ת': 'T'}\n",
    "\n",
    "alphabet_dict_lat_heb = {v:k for k,v in alphabet_dict_heb_lat.items()}\n",
    "\n",
    "alphabet_dict_lat_heb['_'] = ' '\n",
    "alphabet_dict_lat_heb['F'] = 'ש' + 'ׂ'\n",
    "alphabet_dict_lat_heb['/'] = 'ֶ' # nouns/adjectives\n",
    "alphabet_dict_lat_heb['['] = 'ַ' # verbs\n",
    "alphabet_dict_lat_heb['='] = 'ֻ' # lex disambiguation marker\n",
    "\n",
    "new_chars = ['ש', 'ׂ', 'ֶ', 'ַ', 'ֻ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9088482-0441-47b4-9fac-7d67401edab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nme_marker =  '֜'\n",
    "pfm_marker =  'ְ'\n",
    "vbs_marker =  'ֱ'\n",
    "vbe_marker =  'ֲ'\n",
    "prs_marker =  'ֳ'\n",
    "uvf_marker =  'ִ'\n",
    "\n",
    "morpheme_markers = {nme_marker, pfm_marker, vbs_marker, vbe_marker, prs_marker, uvf_marker}\n",
    "\n",
    "# keys indicate indices of morphemes in a word\n",
    "morph_marker_dict = {\n",
    "    4:  '֜',\n",
    "    0:  'ְ',\n",
    "    1:  'ֱ',\n",
    "    3:  'ֲ',\n",
    "    6:  'ֳ',\n",
    "    5:  'ִ'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0eb77da-0307-4095-8118-e9234169a6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'֜',\n",
       " 'ׁ',\n",
       " 'ׂ',\n",
       " 'א',\n",
       " 'ב',\n",
       " 'ג',\n",
       " 'ד',\n",
       " 'ה',\n",
       " 'ו',\n",
       " 'ז',\n",
       " 'ח',\n",
       " 'ט',\n",
       " 'י',\n",
       " 'ך',\n",
       " 'כ',\n",
       " 'ל',\n",
       " 'ם',\n",
       " 'מ',\n",
       " 'ן',\n",
       " 'נ',\n",
       " 'ס',\n",
       " 'ע',\n",
       " 'ף',\n",
       " 'פ',\n",
       " 'ץ',\n",
       " 'צ',\n",
       " 'ק',\n",
       " 'ר',\n",
       " 'ש',\n",
       " 'ת'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chars = set()\n",
    "all_chars_utf8 = set()\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    \n",
    "    morphemes_utf8 = [F.g_lex_utf8.v(w), F.g_nme_utf8.v(w), F.g_pfm_utf8.v(w), F.g_vbs_utf8.v(w), F.g_vbe_utf8.v(w), F.g_uvf_utf8.v(w)] \n",
    "    morphemes_utf8 = [morph if morph else '' for morph in morphemes_utf8]\n",
    "    for morph_utf8 in morphemes_utf8:\n",
    "        all_chars_utf8.update(set(morph_utf8))\n",
    "\n",
    "    morphemes = [F.g_lex_utf8.v(w), F.g_nme_utf8.v(w), F.g_pfm_utf8.v(w), F.g_vbs_utf8.v(w), F.g_vbe_utf8.v(w), F.g_uvf_utf8.v(w)]\n",
    "    morphemes = [morph if morph else '' for morph in morphemes]\n",
    "    for morph in morphemes:\n",
    "        \n",
    "        all_chars.update(set(morph))\n",
    "\n",
    "all_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7170a38-4fd5-4fe9-9228-b4f298488d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_non_overlapping_n_grams(input_list, n):\n",
    "  return [input_list[i:i+n] for i in range(0, len(input_list), n)]\n",
    "\n",
    "def make_n_clause_dict(n):\n",
    "    \"\"\"\n",
    "    Makes sequences of n clauses in the Hebrew Bible, based on a running window.\n",
    "    \"\"\"\n",
    "    n_clause_dict = {}\n",
    "\n",
    "    for bo in F.otype.s('book'):\n",
    "        cl_n_grams = list(make_non_overlapping_n_grams(L.d(bo, 'clause'), n))\n",
    "        \n",
    "        for cl_n_gram in cl_n_grams:\n",
    "            ch = L.u(cl_n_gram[0], 'chapter')[0]\n",
    "            book, chapter_number = T.sectionFromNode(ch)\n",
    "            \n",
    "            words_n_clause = sorted(list(itertools.chain(*[L.d(cl, 'word') for cl in cl_n_gram])))\n",
    "            n_clause_dict[(book, chapter_number, cl_n_gram, 0)] = words_n_clause\n",
    "\n",
    "    return n_clause_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f96eaf93-8a28-462e-877d-69feb0a11f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lex_to_heb_script(tf_word_id):\n",
    "    return ''.join([alphabet_dict_lat_heb[char] for char in F.lex.v(tf_word_id)])\n",
    "\n",
    "def convert_ascii_string_to_heb_script(ascii_string):\n",
    "    return ''.join([alphabet_dict_lat_heb[char] for char in ascii_string])\n",
    "\n",
    "def update_char_dicts(relevant_chars_utf8, alphabet_dict_heb, new_chars):\n",
    "    for new_char in new_chars:\n",
    "        relevant_chars_utf8.add(new_char)\n",
    "        alphabet_dict_heb[new_char] = new_char\n",
    "    return relevant_chars_utf8, alphabet_dict_heb\n",
    "\n",
    "def make_lex_representation_with_verb_noun_marker(tf_id):\n",
    "    lex = F.lex.v(tf_id)\n",
    "    if F.sp.v(tf_id) == 'verb':\n",
    "        lex += '['\n",
    "    elif F.sp.v(tf_id) in {'subs', 'adjv', 'nmpr'}:\n",
    "        lex += '/'\n",
    "    if lex == '=':\n",
    "        lex = ''\n",
    "    return lex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b3645b8-acaf-4332-95ae-b29d46b7db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_word(w, lex_representation, relevant_chars_utf8, alphabet_dict_heb):\n",
    "    if lex_representation == 'g_lex_utf8':\n",
    "        morphs = [F.g_pfm.v(w), F.g_vbs.v(w), F.g_lex.v(w), F.g_vbe.v(w), F.g_nme.v(w), F.g_uvf.v(w)]\n",
    "        morphs = [morph if morph else '' for morph in morphs]\n",
    "    elif lex_representation == 'lex':\n",
    "        lex_rep = make_lex_representation_with_verb_noun_marker(w)\n",
    "        morphs = [F.g_pfm.v(w), F.g_vbs.v(w), lex_rep, F.g_vbe.v(w), F.g_nme.v(w), F.g_uvf.v(w)]\n",
    "        morphs = [morph if morph else '' for morph in morphs]\n",
    "        morphs = [morph.strip('-').strip('!').strip(']').strip('/').strip('[').strip('~') if morphs.index(morph) != 2 else morph for morph in morphs]\n",
    "\n",
    "    morph_list = [''.join([alphabet_dict_lat_heb[char] for char in morph]) for morph in morphs]\n",
    "   \n",
    "    morph_list_with_markers = []\n",
    "    for idx, morph in enumerate(morph_list):\n",
    "        # check if it is prs\n",
    "        if morph and idx == 2 and F.sp.v(w) == 'prps' and not F.g_suffix.v(w-1):\n",
    "            #print(morph)\n",
    "            morph = morph + morph_marker_dict.get(6, '')\n",
    "        elif morph:\n",
    "            morph = morph + morph_marker_dict.get(idx, '')\n",
    "        morph_list_with_markers.append(morph)\n",
    "                \n",
    "    morph_string_with_markers = ' '.join(morph_list_with_markers)\n",
    "    morph_string_with_markers = ' '.join(morph_string_with_markers.split())\n",
    "    \n",
    "    morph_string_without_markers = ' '.join(morph_list)\n",
    "    morph_string_without_markers = ' '.join(morph_string_without_markers.split())\n",
    "\n",
    "    return morph_string_with_markers, morph_string_without_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71438d3f-b1e3-4297-8779-44cab717999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_morpheme_dicts(n_clause_dict, lex_representation, relevant_chars_utf8, alphabet_dict_heb):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    all_morph_strings_with_markers\n",
    "    keys: (book: str, (clause ids))\n",
    "    values: hebrew string with morphemes as separate words (with markers) for morpheme types\n",
    "    \"\"\"\n",
    "    all_morph_strings_with_markers = {}\n",
    "    all_morph_strings_without_markers = {}\n",
    "\n",
    "    for key, words in n_clause_dict.items():\n",
    "        morphemes_in_clause_with_markers = []\n",
    "        morphemes_in_clause_without_markers = []\n",
    "    \n",
    "        for w in words:\n",
    "            morph_string_with_markers, morph_string_without_markers = process_one_word(w, lex_representation, relevant_chars_utf8, alphabet_dict_heb)\n",
    "            \n",
    "            morphemes_in_clause_with_markers.append(morph_string_with_markers)\n",
    "            morphemes_in_clause_without_markers.append(morph_string_without_markers)\n",
    "            \n",
    "        all_morph_strings_with_markers[key] = ' '.join(morphemes_in_clause_with_markers)\n",
    "        all_morph_strings_without_markers[key] = ' '.join(morphemes_in_clause_without_markers)\n",
    "\n",
    "    return all_morph_strings_with_markers, all_morph_strings_without_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b8c5906-4df8-477b-95b2-b7602df58881",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clause_dict = make_n_clause_dict(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e899edd4-9162-462f-bb3c-728b9cae64f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4456"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_clause_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81b4bfbd-f057-4abe-ba57-bc215d5d81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 'lex' or 'g_lex_utf8'\n",
    "# g_lex_utf is the lexeme part of the word, but as it is found in the manuscript. A word can have various spellings\n",
    "# lex is based on the lex feature. This means it has standard spelling and lexeme disambiguation.\n",
    "\n",
    "morpheme_dataset, _ = make_morpheme_dicts(n_clause_dict, 'lex', relevant_chars_utf8, alphabet_dict_heb)\n",
    "\n",
    "hebrew_bible_tokens = set()\n",
    "for token_string in morpheme_dataset.values():\n",
    "    token_set = set(token_string.split())\n",
    "    hebrew_bible_tokens.update(token_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96a540be-94de-45a0-a73e-9093a5beb61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4131"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates in values (= texts)\n",
    "\n",
    "swapped_morpheme_dataset = {v:k for k, v in morpheme_dataset.items()}\n",
    "morpheme_dataset = {v:k for k, v in swapped_morpheme_dataset.items()}\n",
    "len(morpheme_dataset)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c831a0d-aa98-477b-af93-8f6f2ce5b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'xbib_dict_len_{seq_length}.pkl', 'wb') as f:\n",
    "    pickle.dump(morpheme_dataset, f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd042ed-974f-4a41-babd-c2a12a2b7434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
