{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9534ee63-e7b7-4a1d-81c8-a80787f67123",
   "metadata": {},
   "source": [
    "Things to choose:\n",
    "\n",
    "\"g_lex_utf8\" or \"lex\" as representation of the lexeme part of the word.\n",
    "g_lex_utf8 is the words with the prefixes and suffixes removed.\n",
    "lex is th elexeme of the word, with disambiguation signs.\n",
    "lex has the advantage of having standardized spelling and disambiguation.\n",
    "\n",
    "With or without morpheme markers.\n",
    "With morpheme markers it is possible to distinguish between J of the yiqtol and the pronominal suffix. Without markers these are seen as the same token.\n",
    "\n",
    "Maybe:\n",
    "Partly overlapping or no overlapping sequences. The problem with partly overlapping sequences is that parts of the same text occur both in raining and test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71534423-86b1-4d67-b6f1-82884897ca76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Locating corpus resources ...**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local commit\">app:</b> <span title=\"#gb112c161cfd21eae403d51a2733740d8743460e7 offline under C:/Users/geitb/text-fabric-data/github\">~/text-fabric-data/github/etcbc/bhsa/app</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local commit\">data:</b> <span title=\"#gb112c161cfd21eae403d51a2733740d8743460e7 offline under C:/Users/geitb/text-fabric-data/github\">~/text-fabric-data/github/etcbc/bhsa/tf/2021</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local commit\">data:</b> <span title=\"#gaba4367b49750089e4e4122415a77cac43bd97bc offline under C:/Users/geitb/text-fabric-data/github\">~/text-fabric-data/github/etcbc/phono/tf/2021</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local commit\">data:</b> <span title=\"#gf45f6cc3c4f933dba6e649f49cdb14a40dcf333f offline under C:/Users/geitb/text-fabric-data/github\">~/text-fabric-data/github/etcbc/parallels/tf/2021</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local commit\">data:</b> <span title=\"#gb112c161cfd21eae403d51a2733740d8743460e7 offline under C:/Users/geitb/text-fabric-data/github\">~/text-fabric-data/github/etcbc/bhsa/ner</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <b>TF:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"text-fabric api\">TF API 13.0.6</a>, <a target=\"_blank\" href=\"https://github.com/etcbc/bhsa/blob/master/app\" title=\"etcbc/bhsa app\">etcbc/bhsa/app  v3</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/about/searchusage.html\" title=\"Search Templates Introduction and Reference\">Search Reference</a><br>\n",
       "            <b>Data:</b> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">etcbc - bhsa 2021</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/writing/hebrew.html\" title=\"How TF features represent text\">Character table</a>, <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/0_home\" title=\"etcbc - bhsa feature documentation\">Feature docs</a><br>\n",
       "            <details class=\"nodeinfo\"><summary><b>Node types</b></summary>\n",
       "<table class=\"nodeinfo\">\n",
       "    <tr>\n",
       "        <th>Name</th>\n",
       "        <th># of nodes</th>\n",
       "        <th># slots / node</th>\n",
       "        <th>% coverage</th>\n",
       "    </tr>\n",
       "\n",
       "<tr>\n",
       "    <th>book</th>\n",
       "    <td>39</td>\n",
       "    <td>10938.21</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>chapter</th>\n",
       "    <td>929</td>\n",
       "    <td>459.19</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>lex</th>\n",
       "    <td>9230</td>\n",
       "    <td>46.22</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>verse</th>\n",
       "    <td>23213</td>\n",
       "    <td>18.38</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>half_verse</th>\n",
       "    <td>45179</td>\n",
       "    <td>9.44</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>sentence</th>\n",
       "    <td>63717</td>\n",
       "    <td>6.70</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>sentence_atom</th>\n",
       "    <td>64514</td>\n",
       "    <td>6.61</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>clause</th>\n",
       "    <td>88131</td>\n",
       "    <td>4.84</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>clause_atom</th>\n",
       "    <td>90704</td>\n",
       "    <td>4.70</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>phrase</th>\n",
       "    <td>253203</td>\n",
       "    <td>1.68</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>phrase_atom</th>\n",
       "    <td>267532</td>\n",
       "    <td>1.59</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th>subphrase</th>\n",
       "    <td>113850</td>\n",
       "    <td>1.42</td>\n",
       "    <td>38</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "    <th><i>word</i></th>\n",
       "    <td>426590</td>\n",
       "    <td>1.00</td>\n",
       "    <td><b>100</b></td>\n",
       "</tr>\n",
       "</table></details>\n",
       "            <b>Sets:</b> no custom sets<br>\n",
       "            <b>Features:</b><br>\n",
       "<details><summary><b>Parallel Passages</b></summary>\n",
       "    <div class=\"fcorpus\">\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat edge\">\n",
       "<a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"~/text-fabric-data/github/etcbc/parallels/tf/2021/crossref.tf\">crossref</a>\n",
       "</div>\n",
       "<div class=\"fmono\">int</div>\n",
       "\n",
       "<span> 🆗 links between similar passages</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "</details>\n",
       "\n",
       "<details><summary><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b></summary>\n",
       "    <div class=\"fcorpus\">\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/book.tf\">book</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ book name in Latin (Genesis; Numeri; Reges1; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/book@ll\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/book@am.tf\">book@ll</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ book name in amharic (ኣማርኛ)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/chapter\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/chapter.tf\">chapter</a>\n",
       "</div>\n",
       "<div class=\"fmono\">int</div>\n",
       "\n",
       "<span> ✅ chapter number (1; 2; 3; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/code\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/code.tf\">code</a>\n",
       "</div>\n",
       "<div class=\"fmono\">int</div>\n",
       "\n",
       "<span> ✅ identifier of a clause atom relationship (0; 74; 367; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/det\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/det.tf\">det</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ determinedness of phrase(atom) (det; und; NA.)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/domain\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/domain.tf\">domain</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ text type of clause (? (Unknown); N (narrative); D (discursive); Q (Quotation).)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/freq_lex\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/freq_lex.tf\">freq_lex</a>\n",
       "</div>\n",
       "<div class=\"fmono\">int</div>\n",
       "\n",
       "<span> ✅ frequency of lexemes</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/function\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/function.tf\">function</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ syntactic function of phrase (Cmpl; Objc; Pred; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/g_cons.tf\">g_cons</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ word consonantal-transliterated (B R>CJT BR> >LHJM ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_cons_utf8\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/g_cons_utf8.tf\">g_cons_utf8</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ word consonantal-Hebrew (ב ראשׁית ברא אלהים)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/g_lex.tf\">g_lex</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ lexeme pointed-transliterated (B.:- R;>CIJT B.@R@> >:ELOH ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_lex_utf8\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/g_lex_utf8.tf\">g_lex_utf8</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ lexeme pointed-Hebrew (בְּ רֵאשִׁית בָּרָא אֱלֹה)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/g_word.tf\">g_word</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ word pointed-transliterated (B.:- R;>CI73JT B.@R@74> >:ELOHI92JM)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/g_word_utf8\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/g_word_utf8.tf\">g_word_utf8</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ word pointed-Hebrew (בְּ רֵאשִׁ֖ית בָּרָ֣א אֱלֹהִ֑ים)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gloss\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/gloss.tf\">gloss</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> 🆗 english translation of lexeme (beginning create god(s))</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/gn\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/gn.tf\">gn</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ grammatical gender (m; f; NA; unknown.)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/label\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/label.tf\">label</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ (half-)verse label (half verses: A; B; C; verses:  GEN 01,02)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/language\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/language.tf\">language</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ of word or lexeme (Hebrew; Aramaic.)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/lex.tf\">lex</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ lexeme consonantal-transliterated (B R>CJT/ BR>[ >LHJM/)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/lex_utf8\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/lex_utf8.tf\">lex_utf8</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ lexeme consonantal-Hebrew (ב ראשׁית֜ ברא אלהים֜)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ls\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/ls.tf\">ls</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ lexical set, subclassification of part-of-speech (card; ques; mult)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nametype\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/nametype.tf\">nametype</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ⚠️ named entity type (pers; mens; gens; topo; ppde.)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nme\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/nme.tf\">nme</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ nominal ending consonantal-transliterated (absent; n/a; JM, ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/nu\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/nu.tf\">nu</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ grammatical number (sg; du; pl; NA; unknown.)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/number\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/number.tf\">number</a>\n",
       "</div>\n",
       "<div class=\"fmono\">int</div>\n",
       "\n",
       "<span> ✅ sequence number of an object within its context</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/otype\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/otype.tf\">otype</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> </span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pargr\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/pargr.tf\">pargr</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> 🆗 hierarchical paragraph number (1; 1.2; 1.2.3.4; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pdp\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/pdp.tf\">pdp</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ phrase dependent part-of-speech (art; verb; subs; nmpr, ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/pfm\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/pfm.tf\">pfm</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ preformative consonantal-transliterated (absent; n/a; J, ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/prs.tf\">prs</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ pronominal suffix consonantal-transliterated (absent; n/a; W; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_gn\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/prs_gn.tf\">prs_gn</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ pronominal suffix gender (m; f; NA; unknown.)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_nu\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/prs_nu.tf\">prs_nu</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ pronominal suffix number (sg; du; pl; NA; unknown.)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/prs_ps\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/prs_ps.tf\">prs_ps</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ pronominal suffix person (p1; p2; p3; NA; unknown.)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/ps\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/ps.tf\">ps</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ grammatical person (p1; p2; p3; NA; unknown.)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/qere.tf\">qere</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ word pointed-transliterated masoretic reading correction</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/qere_trailer.tf\">qere_trailer</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ interword material -pointed-transliterated (Masoretic correction)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_trailer_utf8\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/qere_trailer_utf8.tf\">qere_trailer_utf8</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ interword material -pointed-transliterated (Masoretic correction)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/qere_utf8\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/qere_utf8.tf\">qere_utf8</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ word pointed-Hebrew masoretic reading correction</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rank_lex\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/rank_lex.tf\">rank_lex</a>\n",
       "</div>\n",
       "<div class=\"fmono\">int</div>\n",
       "\n",
       "<span> ✅ ranking of lexemes based on freqnuecy</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/rela\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/rela.tf\">rela</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ linguistic relation between clause/(sub)phrase(atom) (ADJ; MOD; ATR; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/sp\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/sp.tf\">sp</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ part-of-speech (art; verb; subs; nmpr, ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/st\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/st.tf\">st</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ state of a noun (a (absolute); c (construct); e (emphatic).)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/tab\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/tab.tf\">tab</a>\n",
       "</div>\n",
       "<div class=\"fmono\">int</div>\n",
       "\n",
       "<span> ✅ clause atom: its level in the linguistic embedding</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/trailer.tf\">trailer</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ interword material pointed-transliterated (& 00 05 00_P ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/trailer_utf8\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/trailer_utf8.tf\">trailer_utf8</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ interword material pointed-Hebrew (־ ׃)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/txt\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/txt.tf\">txt</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ text type of clause and surrounding (repetion of ? N D Q as in feature domain)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/typ\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/typ.tf\">typ</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ clause/phrase(atom) type (VP; NP; Ellp; Ptcp; WayX)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/uvf\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/uvf.tf\">uvf</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ univalent final consonant consonantal-transliterated (absent; N; J; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbe\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/vbe.tf\">vbe</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ verbal ending consonantal-transliterated (n/a; W; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vbs\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/vbs.tf\">vbs</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ root formation consonantal-transliterated (absent; n/a; H; ...)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/verse\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/verse.tf\">verse</a>\n",
       "</div>\n",
       "<div class=\"fmono\">int</div>\n",
       "\n",
       "<span> ✅ verse number</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/voc_lex.tf\">voc_lex</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ vocalized lexeme pointed-transliterated (B.: R;>CIJT BR> >:ELOHIJM)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/voc_lex_utf8\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/voc_lex_utf8.tf\">voc_lex_utf8</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ vocalized lexeme pointed-Hebrew (בְּ רֵאשִׁית ברא אֱלֹהִים)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vs\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/vs.tf\">vs</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ verbal stem (qal; piel; hif; apel; pael)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/vt\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/vt.tf\">vt</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> ✅ verbal tense (perf; impv; wayq; infc)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat edge\">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/mother\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/mother.tf\">mother</a>\n",
       "</div>\n",
       "<div class=\"fmono\">none</div>\n",
       "\n",
       "<span> ✅ linguistic dependency between textual objects</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat edge\">\n",
       "<a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/oslots\" title=\"~/text-fabric-data/github/etcbc/bhsa/tf/2021/oslots.tf\">oslots</a>\n",
       "</div>\n",
       "<div class=\"fmono\">none</div>\n",
       "\n",
       "<span> </span>\n",
       "\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "</details>\n",
       "\n",
       "<details><summary><b>Phonetic Transcriptions</b></summary>\n",
       "    <div class=\"fcorpus\">\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/github/etcbc/phono/tf/2021/phono.tf\">phono</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> 🆗 phonological transcription (bᵊ rēšˌîṯ bārˈā ʔᵉlōhˈîm)</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"frow\">\n",
       "    <div class=\"fnamecat \">\n",
       "<a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"~/text-fabric-data/github/etcbc/phono/tf/2021/phono_trailer.tf\">phono_trailer</a>\n",
       "</div>\n",
       "<div class=\"fmono\">str</div>\n",
       "\n",
       "<span> 🆗 interword material in phonological transcription</span>\n",
       "\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "</details>\n",
       "\n",
       "            <b>Settings:</b><br><details ><summary><b>specified</b></summary><ol><li><b>apiVersion</b>: <code>3</code></li><li><b>appName</b>: <code>etcbc/bhsa</code></li><li><b>appPath</b>: <code>C:/Users/geitb/text-fabric-data/github/etcbc/bhsa/app</code></li><li><b>commit</b>: <code>gb112c161cfd21eae403d51a2733740d8743460e7</code></li><li><b>css</b>: <code>''</code></li><li><details><summary><b>dataDisplay</b>:</summary><ul><li><details><summary><b>exampleSectionHtml</b>:</summary><code>&lt;code&gt;Genesis 1:1&lt;/code&gt; (use &lt;a href=\"https://github.com/{org}/{repo}/blob/master/tf/{version}/book%40en.tf\" target=\"_blank\"&gt;English book names&lt;/a&gt;)</code></details></li><li><details><summary><b>excludedFeatures</b>:</summary><ul><li><code>g_uvf_utf8</code></li><li><code>g_vbs</code></li><li><code>kq_hybrid</code></li><li><code>languageISO</code></li><li><code>g_nme</code></li><li><code>lex0</code></li><li><code>is_root</code></li><li><code>g_vbs_utf8</code></li><li><code>g_uvf</code></li><li><code>dist</code></li><li><code>root</code></li><li><code>suffix_person</code></li><li><code>g_vbe</code></li><li><code>dist_unit</code></li><li><code>suffix_number</code></li><li><code>distributional_parent</code></li><li><code>kq_hybrid_utf8</code></li><li><code>crossrefSET</code></li><li><code>instruction</code></li><li><code>g_prs</code></li><li><code>lexeme_count</code></li><li><code>rank_occ</code></li><li><code>g_pfm_utf8</code></li><li><code>freq_occ</code></li><li><code>crossrefLCS</code></li><li><code>functional_parent</code></li><li><code>g_pfm</code></li><li><code>g_nme_utf8</code></li><li><code>g_vbe_utf8</code></li><li><code>kind</code></li><li><code>g_prs_utf8</code></li><li><code>suffix_gender</code></li><li><code>mother_object_type</code></li></ul></details></li><li><details><summary><b>noneValues</b>:</summary><ul><li><code>absent</code></li><li><code>n/a</code></li><li><code>none</code></li><li><code>unknown</code></li><li><i>no value</i></li><li><code>NA</code></li></ul></details></li></ul></details></li><li><details><summary><b>docs</b>:</summary><ul><li><b>docBase</b>: <code>{docRoot}/{repo}</code></li><li><b>docExt</b>: <code>''</code></li><li><b>docPage</b>: <code>''</code></li><li><b>docRoot</b>: <code>https://{org}.github.io</code></li><li><b>featurePage</b>: <code>0_home</code></li></ul></details></li><li><b>interfaceDefaults</b>: <code>{}</code></li><li><b>isCompatible</b>: <code>True</code></li><li><b>local</b>: <code>local</code></li><li><b>localDir</b>: <code>C:/Users/geitb/text-fabric-data/github/etcbc/bhsa/_temp</code></li><li><details><summary><b>provenanceSpec</b>:</summary><ul><li><b>corpus</b>: <code>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</code></li><li><b>doi</b>: <code>10.5281/zenodo.1007624</code></li><li><b>extraData</b>: <code>ner</code></li><li><details><summary><b>moduleSpecs</b>:</summary><ul><li><details><summary>:</summary><ul><li><b>backend</b>: <i>no value</i></li><li><b>corpus</b>: <code>Phonetic Transcriptions</code></li><li><details><summary><b>docUrl</b>:</summary><code>https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb</code></details></li><li><b>doi</b>: <code>10.5281/zenodo.1007636</code></li><li><b>org</b>: <code>etcbc</code></li><li><b>relative</b>: <code>/tf</code></li><li><b>repo</b>: <code>phono</code></li></ul></details></li><li><details><summary>:</summary><ul><li><b>backend</b>: <i>no value</i></li><li><b>corpus</b>: <code>Parallel Passages</code></li><li><details><summary><b>docUrl</b>:</summary><code>https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb</code></details></li><li><b>doi</b>: <code>10.5281/zenodo.1007642</code></li><li><b>org</b>: <code>etcbc</code></li><li><b>relative</b>: <code>/tf</code></li><li><b>repo</b>: <code>parallels</code></li></ul></details></li></ul></details></li><li><b>org</b>: <code>etcbc</code></li><li><b>relative</b>: <code>/tf</code></li><li><b>repo</b>: <code>bhsa</code></li><li><b>version</b>: <code>2021</code></li><li><b>webBase</b>: <code>https://shebanq.ancient-data.org/hebrew</code></li><li><b>webHint</b>: <code>Show this on SHEBANQ</code></li><li><b>webLang</b>: <code>la</code></li><li><b>webLexId</b>: <code>True</code></li><li><details><summary><b>webUrl</b>:</summary><code>{webBase}/text?book=&lt;1&gt;&amp;chapter=&lt;2&gt;&amp;verse=&lt;3&gt;&amp;version={version}&amp;mr=m&amp;qw=q&amp;tp=txt_p&amp;tr=hb&amp;wget=v&amp;qget=v&amp;nget=vt</code></details></li><li><b>webUrlLex</b>: <code>{webBase}/word?version={version}&amp;id=&lt;lid&gt;</code></li></ul></details></li><li><b>release</b>: <i>no value</i></li><li><details><summary><b>typeDisplay</b>:</summary><ul><li><details><summary><b>clause</b>:</summary><ul><li><b>label</b>: <code>{typ} {rela}</code></li><li><b>style</b>: <code>''</code></li></ul></details></li><li><details><summary><b>clause_atom</b>:</summary><ul><li><b>hidden</b>: <code>True</code></li><li><b>label</b>: <code>{code}</code></li><li><b>level</b>: <code>1</code></li><li><b>style</b>: <code>''</code></li></ul></details></li><li><details><summary><b>half_verse</b>:</summary><ul><li><b>hidden</b>: <code>True</code></li><li><b>label</b>: <code>{label}</code></li><li><b>style</b>: <code>''</code></li><li><b>verselike</b>: <code>True</code></li></ul></details></li><li><details><summary><b>lex</b>:</summary><ul><li><b>featuresBare</b>: <code>gloss</code></li><li><b>label</b>: <code>{voc_lex_utf8}</code></li><li><b>lexOcc</b>: <code>word</code></li><li><b>style</b>: <code>orig</code></li><li><b>template</b>: <code>{voc_lex_utf8}</code></li></ul></details></li><li><details><summary><b>phrase</b>:</summary><ul><li><b>label</b>: <code>{typ} {function}</code></li><li><b>style</b>: <code>''</code></li></ul></details></li><li><details><summary><b>phrase_atom</b>:</summary><ul><li><b>hidden</b>: <code>True</code></li><li><b>label</b>: <code>{typ} {rela}</code></li><li><b>level</b>: <code>1</code></li><li><b>style</b>: <code>''</code></li></ul></details></li><li><details><summary><b>sentence</b>:</summary><ul><li><b>label</b>: <code>{number}</code></li><li><b>style</b>: <code>''</code></li></ul></details></li><li><details><summary><b>sentence_atom</b>:</summary><ul><li><b>hidden</b>: <code>True</code></li><li><b>label</b>: <code>{number}</code></li><li><b>level</b>: <code>1</code></li><li><b>style</b>: <code>''</code></li></ul></details></li><li><details><summary><b>subphrase</b>:</summary><ul><li><b>hidden</b>: <code>True</code></li><li><b>label</b>: <code>{number}</code></li><li><b>style</b>: <code>''</code></li></ul></details></li><li><details><summary><b>word</b>:</summary><ul><li><b>features</b>: <code>pdp vs vt</code></li><li><b>featuresBare</b>: <code>lex:gloss</code></li></ul></details></li></ul></details></li><li><b>writing</b>: <code>hbo</code></li></ol></details>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Gentium Plus\";\n",
       "  src: local('Gentium Plus'), local('GentiumPlus'),\n",
       "    url('/browser/static/fonts/GentiumPlus-R.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/browser/static/fonts/GentiumPlus-R.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/browser/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/browser/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/browser/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/browser/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/browser/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/browser/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/browser/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/browser/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/browser/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/browser/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/browser/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/browser/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/browser/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/browser/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".txtu.uga,.lex.uga {\n",
       "    font-family: sans-serif;\n",
       "    /*font-family: Noto Sans Ugaritic, sans-serif;*/\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".ubd {\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".col {\n",
       "   display: inline-block;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 2px;\n",
       "    margin: 2px;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -2px 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 2px 0px;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".tfsechead {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--tfsechead);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".nde, a:link.nde {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".etf {\n",
       "    font-size: normal;\n",
       "    border-radius: 0.2rem;\n",
       "    border: 1pt solid white;\n",
       "    padding: 0 0.2rem ! important;\n",
       "    margin: 0 0.2rem ! important;\n",
       "}\n",
       ".etfx {\n",
       "    font-size: x-large;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       "#colormapplus, #colormapmin, .ecolormapmin {\n",
       "  font-weight: bold;\n",
       "  border-radius: 0.1rem;\n",
       "  background-color: #eeeeff;\n",
       "  padding: 0 1rem;\n",
       "  margin: 0 1rem;\n",
       "}\n",
       ".clr {\n",
       "  font-style: italic;\n",
       "  font-size: small;\n",
       "}\n",
       ".clmap,.eclmap {\n",
       "  padding: 0;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  10px 2px 2px 2px;\n",
       "    margin: 16px 2px 2px 2px;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    /*display: inline-block;*/\n",
       "    display: inline-flex;\n",
       "    flex-flow: row wrap;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       "span.break {\n",
       "  flex-basis: 100%;\n",
       "  height: 0;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 4px;\n",
       "    margin-left: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 4px;\n",
       "    margin-right: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -24px;\n",
       "    margin-left: 20px;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 6px;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 2px;\n",
       "    margin-left: 2px;\n",
       "    padding: 2px 2px;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 40px;\n",
       "\tpadding: 20px;\n",
       "\tborder: 2px solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       "/* KEYBOARD */\n",
       ".ccoff {\n",
       "  background-color: inherit;\n",
       "}\n",
       ".ccon {\n",
       "  background-color: yellow ! important;\n",
       "}\n",
       ".ccon,.ccoff {\n",
       "  padding: 0.2rem;\n",
       "  margin: 0.2rem;\n",
       "  border: 0.1rem solid var(--letter-box-border);\n",
       "  border-radius: 0.1rem;\n",
       "}\n",
       ".ccline {\n",
       "  font-size: xx-large ! important;\n",
       "  font-weight: bold;\n",
       "  line-height: 2em ! important;\n",
       "}\n",
       "/* TF header */\n",
       "\n",
       "summary {\n",
       "  /* needed to override the normalize.less\n",
       "   * in the classical Jupyter Notebook\n",
       "   */\n",
       "  display: list-item ! important;\n",
       "}\n",
       "\n",
       ".fcorpus {\n",
       "  display: flex;\n",
       "  flex-flow: column nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: flex-start;\n",
       "  align-content: flex-start;\n",
       "  overflow: auto;\n",
       "}\n",
       ".frow {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: flex-start;\n",
       "  align-content: flex-start;\n",
       "}\n",
       ".fmeta {\n",
       "  display: flex;\n",
       "  flex-flow: column nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: flex-start;\n",
       "  align-content: flex-start;\n",
       "}\n",
       ".fmetarow {\n",
       "  display: flex;\n",
       "  flex-flow: row nowrap;\n",
       "  justify-content: flex-start;\n",
       "  align-items: flex-start;\n",
       "  align-content: flex-start;\n",
       "}\n",
       ".fmetakey {\n",
       "  min-width: 8em;\n",
       "  font-family: monospace;\n",
       "}\n",
       ".fnamecat {\n",
       "  min-width: 8em;\n",
       "}\n",
       ".fnamecat.edge {\n",
       "  font-weight: bold;\n",
       "  font-style: italic;\n",
       "}\n",
       ".fmono {\n",
       "    font-family: monospace;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--tfsechead:          hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         3px;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "\t--letter-box-border:  hsla(  0,   0%,  80%, 0.5  );\n",
       "  --border-width-nul:   2px;\n",
       "  --border-width0:      2px;\n",
       "  --border-width1:      3px;\n",
       "  --border-width2:      4px;\n",
       "  --border-width3:      6px;\n",
       "  --border-width4:      5px;\n",
       "  --border-width-plain: 2px;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 2px;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 4px ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 4px ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 6px;\n",
       "  padding: 4px;\n",
       "  margin: 4px;\n",
       "}\n",
       ".ehl {\n",
       "  background-color: var(--ehl-strong);\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "\t--ehl-strong:       hsla(240, 100%,  70%, 0.9  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "globalThis.copyChar = (el, c) => {\n",
       "    for (const el of document.getElementsByClassName('ccon')) {\n",
       "        el.className = 'ccoff'\n",
       "    }\n",
       "    el.className = 'ccon'\n",
       "    navigator.clipboard.writeText(String.fromCharCode(c))\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><b>TF API:</b> names <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"doc\">N F E L T S C TF Fs Fall Es Eall Cs Call</a> directly usable</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections, itertools, os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    BertConfig, \n",
    "    BertModel, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    AutoModelForMaskedLM,\n",
    "    BertForMaskedLM,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "from tf.app import use\n",
    "A = use('etcbc/bhsa', hoist=globals())\n",
    "\n",
    "A.load([\n",
    "        'g_lex_utf8', 'g_prs_utf8', 'g_nme_utf8', 'g_pfm_utf8', 'g_vbs_utf8', 'g_vbe_utf8', 'g_uvf_utf8'\n",
    "       ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5984bea-81a0-4fb2-bae2-94b4f7fa1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './models'\n",
    "seq_length = 5\n",
    "augment_factor = 10\n",
    "\n",
    "# model details\n",
    "num_hidden_layers = 6\n",
    "num_attention_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ab9564-621b-484e-a58d-9059f3e6dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_chars_utf8 = {' ',\n",
    " 'א',\n",
    " 'ב',\n",
    " 'ג',\n",
    " 'ד',\n",
    " 'ה',\n",
    " 'ו',\n",
    " 'ז',\n",
    " 'ח',\n",
    " 'ט',\n",
    " 'י',\n",
    " 'ך',\n",
    " 'כ',\n",
    " 'ל',\n",
    " 'ם',\n",
    " 'מ',\n",
    " 'ן',\n",
    " 'נ',\n",
    " 'ס',\n",
    " 'ע',\n",
    " 'ף',\n",
    " 'פ',\n",
    " 'ץ',\n",
    " 'צ',\n",
    " 'ק',\n",
    " 'ר',\n",
    " 'ש',\n",
    " 'ת'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b661bb6-e68d-4d9c-84f0-ebf841e33882",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet_dict_heb = {char: char for char in relevant_chars_utf8}\n",
    "double_chars = ['ןנ','ףפ', 'ץצ','ךכ','םמ']\n",
    "\n",
    "for end_char, non_end_char in double_chars:\n",
    "    alphabet_dict_heb[end_char] = non_end_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f343aa-f9d8-4774-b33e-1a7430973028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for conversion of lex to hebrew script, including markers =, / and [\n",
    "\n",
    "alphabet_dict_heb_lat = {'א': '>',\n",
    "                                      'ב': 'B',\n",
    "                                      'ג': 'G',\n",
    "                                      'ד': 'D',\n",
    "                                      'ה': 'H',\n",
    "                                      'ו': 'W',\n",
    "                                      'ז': 'Z',\n",
    "                                      'ח': 'X',\n",
    "                                      'ט': 'V',\n",
    "                                      'י': 'J',\n",
    "                                      'כ': 'K',\n",
    "                                      'ל': 'L',\n",
    "                                      'מ': 'M',\n",
    "                                      'נ': 'N',\n",
    "                                      'ס': 'S',\n",
    "                                      'ע': '<',\n",
    "                                      'פ': 'P',\n",
    "                                      'צ': 'Y',\n",
    "                                      'ק': 'Q',\n",
    "                                      'ר': 'R',\n",
    "                                      'ש': 'C',\n",
    "                                      'ת': 'T'}\n",
    "\n",
    "alphabet_dict_lat_heb = {v:k for k,v in alphabet_dict_heb_lat.items()}\n",
    "\n",
    "alphabet_dict_lat_heb['_'] = ' '\n",
    "alphabet_dict_lat_heb['F'] = 'ש' + 'ׂ'\n",
    "alphabet_dict_lat_heb['/'] = 'ֶ' # nouns/adjectives\n",
    "alphabet_dict_lat_heb['['] = 'ַ' # verbs\n",
    "alphabet_dict_lat_heb['='] = 'ֻ' # lex disambiguation marker\n",
    "\n",
    "new_chars = ['ש', 'ׂ', 'ֶ', 'ַ', 'ֻ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1380ae99-8fa9-4ceb-ac26-a6e625e85070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are morphemes together always identical with g_cons? \n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    g_cons = ''.join([alphabet_dict_heb[char] for char in F.g_cons_utf8.v(w) if char in relevant_chars_utf8])\n",
    "    morphs = [F.g_pfm_utf8.v(w), F.g_vbs_utf8.v(w), F.g_lex_utf8.v(w), F.g_vbe_utf8.v(w), F.g_nme_utf8.v(w), F.g_uvf_utf8.v(w), F.g_prs_utf8.v(w)]\n",
    "    reconstr = ''.join([''.join([alphabet_dict_heb[char] for char in morph if char in relevant_chars_utf8]) for morph in morphs])\n",
    "    if g_cons != reconstr:\n",
    "        print(w, g_cons, reconstr, F.g_uvf_utf8.v(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9088482-0441-47b4-9fac-7d67401edab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nme_marker =  '֜'\n",
    "pfm_marker =  'ְ'\n",
    "vbs_marker =  'ֱ'\n",
    "vbe_marker =  'ֲ'\n",
    "prs_marker =  'ֳ'\n",
    "uvf_marker =  'ִ'\n",
    "\n",
    "morpheme_markers = {nme_marker, pfm_marker, vbs_marker, vbe_marker, prs_marker, uvf_marker}\n",
    "\n",
    "# keys indicate indices of morphemes in a word\n",
    "morph_marker_dict = {\n",
    "    4:  '֜',\n",
    "    0:  'ְ',\n",
    "    1:  'ֱ',\n",
    "    3:  'ֲ',\n",
    "    6:  'ֳ',\n",
    "    5:  'ִ'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0eb77da-0307-4095-8118-e9234169a6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '֜',\n",
       " 'ְ',\n",
       " 'ֱ',\n",
       " 'ֲ',\n",
       " 'ֳ',\n",
       " 'ִ',\n",
       " 'ֵ',\n",
       " 'ֶ',\n",
       " 'ַ',\n",
       " 'ָ',\n",
       " 'ֹ',\n",
       " 'ֻ',\n",
       " 'ּ',\n",
       " 'ֿ',\n",
       " 'ׁ',\n",
       " 'ׂ',\n",
       " 'א',\n",
       " 'ב',\n",
       " 'ג',\n",
       " 'ד',\n",
       " 'ה',\n",
       " 'ו',\n",
       " 'ז',\n",
       " 'ח',\n",
       " 'ט',\n",
       " 'י',\n",
       " 'ך',\n",
       " 'כ',\n",
       " 'ל',\n",
       " 'ם',\n",
       " 'מ',\n",
       " 'ן',\n",
       " 'נ',\n",
       " 'ס',\n",
       " 'ע',\n",
       " 'ף',\n",
       " 'פ',\n",
       " 'ץ',\n",
       " 'צ',\n",
       " 'ק',\n",
       " 'ר',\n",
       " 'ש',\n",
       " 'ת'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chars = set()\n",
    "all_chars_utf8 = set()\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    \n",
    "    morphemes_utf8 = [F.g_lex_utf8.v(w), F.g_nme_utf8.v(w), F.g_pfm_utf8.v(w), F.g_vbs_utf8.v(w), F.g_vbe_utf8.v(w), F.g_uvf_utf8.v(w), F.g_prs_utf8.v(w)]\n",
    "    for morph_utf8 in morphemes_utf8:\n",
    "        all_chars_utf8.update(set(morph_utf8))\n",
    "\n",
    "    morphemes = [F.g_lex_utf8.v(w), F.g_nme_utf8.v(w), F.g_pfm_utf8.v(w), F.g_vbs_utf8.v(w), F.g_vbe_utf8.v(w), F.g_prs_utf8.v(w), F.g_uvf_utf8.v(w)]\n",
    "    for morph in morphemes:\n",
    "        \n",
    "        all_chars.update(set(morph))\n",
    "\n",
    "all_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7170a38-4fd5-4fe9-9228-b4f298488d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_non_overlapping_n_grams(input_list, n):\n",
    "  return [input_list[i:i+n] for i in range(0, len(input_list), n)]  #zip(*[input_list[i:] for i in range(n)])\n",
    "\n",
    "def make_n_clause_dict(n):\n",
    "    \"\"\"\n",
    "    Makes sequences of n clauses in the Hebrew Bible, based on a running window.\n",
    "    \"\"\"\n",
    "    n_clause_dict = {}\n",
    "\n",
    "    for bo in F.otype.s('book'):\n",
    "        cl_n_grams = list(make_non_overlapping_n_grams(L.d(bo, 'clause'), n))\n",
    "        \n",
    "        for cl_n_gram in cl_n_grams:\n",
    "            ch = L.u(cl_n_gram[0], 'chapter')[0]\n",
    "            book, chapter_number = T.sectionFromNode(ch)\n",
    "            \n",
    "            words_n_clause = sorted(list(itertools.chain(*[L.d(cl, 'word') for cl in cl_n_gram])))\n",
    "            n_clause_dict[(book, chapter_number, cl_n_gram, 0)] = words_n_clause\n",
    "\n",
    "    return n_clause_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b3645b8-acaf-4332-95ae-b29d46b7db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one_word(w, lex_representation, relevant_chars_utf8, alphabet_dict_heb):\n",
    "    if lex_representation == 'g_lex_utf8':\n",
    "        morphs = [F.g_pfm_utf8.v(w), F.g_vbs_utf8.v(w), F.g_lex_utf8.v(w), F.g_vbe_utf8.v(w), F.g_nme_utf8.v(w), F.g_uvf_utf8.v(w), F.g_prs_utf8.v(w)]\n",
    "    elif lex_representation == 'lex':\n",
    "        lex_rep = convert_lex_to_heb_script(w)\n",
    "        morphs = [F.g_pfm_utf8.v(w), F.g_vbs_utf8.v(w), lex_rep, F.g_vbe_utf8.v(w), F.g_nme_utf8.v(w), F.g_uvf_utf8.v(w), F.g_prs_utf8.v(w)]\n",
    "        relevant_chars_utf8, alphabet_dict_heb = update_char_dicts(relevant_chars_utf8, alphabet_dict_heb, new_chars)\n",
    "\n",
    "    morph_list = [''.join([alphabet_dict_heb[char] for char in morph if char in relevant_chars_utf8]) for morph in morphs]\n",
    "        \n",
    "    morph_list_with_markers = []\n",
    "    for idx, morph in enumerate(morph_list):\n",
    "        if morph:\n",
    "            morph = morph + morph_marker_dict.get(idx, '')\n",
    "            morph_list_with_markers.append(morph)\n",
    "                \n",
    "    morph_string_with_markers = ' '.join(morph_list_with_markers)\n",
    "    morph_string_with_markers = ' '.join(morph_string_with_markers.split())\n",
    "    \n",
    "    morph_string_without_markers = ' '.join(morph_list)\n",
    "    morph_string_without_markers = ' '.join(morph_string_without_markers.split())\n",
    "\n",
    "    return morph_string_with_markers, morph_string_without_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71438d3f-b1e3-4297-8779-44cab717999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lex_to_heb_script(tf_word_id):\n",
    "    return ''.join([alphabet_dict_lat_heb[char] for char in F.lex.v(tf_word_id)])\n",
    "\n",
    "def update_char_dicts(relevant_chars_utf8, alphabet_dict_heb, new_chars):\n",
    "    for new_char in new_chars:\n",
    "        relevant_chars_utf8.add(new_char)\n",
    "        alphabet_dict_heb[new_char] = new_char\n",
    "    return relevant_chars_utf8, alphabet_dict_heb\n",
    "    \n",
    "\n",
    "def make_morpheme_dicts(n_clause_dict, lex_representation, relevant_chars_utf8, alphabet_dict_heb):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    all_morph_strings_with_markers\n",
    "    keys: (book: str, (clause ids))\n",
    "    values: hebrew string with morphemes as separate words (with markers) for morpheme types\n",
    "    \"\"\"\n",
    "    all_morph_strings_with_markers = {}\n",
    "    all_morph_strings_without_markers = {}\n",
    "\n",
    "    for key, words in n_clause_dict.items():\n",
    "        morphemes_in_clause_with_markers = []\n",
    "        morphemes_in_clause_without_markers = []\n",
    "    \n",
    "        for w in words:\n",
    "            morph_string_with_markers, morph_string_without_markers = process_one_word(w, lex_representation, relevant_chars_utf8, alphabet_dict_heb)\n",
    "            \n",
    "            morphemes_in_clause_with_markers.append(morph_string_with_markers)\n",
    "            morphemes_in_clause_without_markers.append(morph_string_without_markers)\n",
    "\n",
    "        all_morph_strings_with_markers[key] = ' '.join(morphemes_in_clause_with_markers)\n",
    "        all_morph_strings_without_markers[key] = ' '.join(morphemes_in_clause_without_markers)\n",
    "\n",
    "    return all_morph_strings_with_markers, all_morph_strings_without_markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b8c5906-4df8-477b-95b2-b7602df58881",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clause_dict = make_n_clause_dict(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e899edd4-9162-462f-bb3c-728b9cae64f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17640"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_clause_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41ac28a7-d8db-49bc-91ca-202ffe2d871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_names = {w: F.lex.v(w) for w in F.otype.s('word') if F.nametype.v(w) == 'pers'}\n",
    "person_names_swapped = {v:k for k, v in person_names.items()}\n",
    "person_names_swapped = {v:k for k, v in person_names.items()}\n",
    "unique_person_ids = list(person_names_swapped.values())\n",
    "\n",
    "topo_names = {w: F.lex.v(w) for w in F.otype.s('word') if F.nametype.v(w) == 'topo'}\n",
    "topo_names_swapped = {v:k for k, v in topo_names.items()}\n",
    "unique_topo_ids = list(topo_names_swapped.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37d79a02-ca76-47c9-8634-71afa1d97c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176400"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "n_clause_dict_augmented = {}\n",
    "\n",
    "for key, words in n_clause_dict.items():\n",
    "    bo, ch, cl, nr = key\n",
    "    n_clause_dict_augmented[key] = words\n",
    "    words_copy = words.copy()\n",
    "    for i in range(1, augment_factor):\n",
    "        for idx, tf_id in enumerate(words):\n",
    "            if  F.nametype.v(tf_id) == 'pers':\n",
    "                new_name_id = random.choice(unique_person_ids)\n",
    "                words_copy[idx] = new_name_id\n",
    "            elif F.nametype.v(tf_id) == 'topo':\n",
    "                new_topo_id = random.choice(unique_topo_ids)\n",
    "                words_copy[idx] = new_topo_id\n",
    "    \n",
    "        key_i = (bo, ch, cl, i)\n",
    "        n_clause_dict_augmented[key_i] = words_copy\n",
    "\n",
    "len(n_clause_dict_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81b4bfbd-f057-4abe-ba57-bc215d5d81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 'lex' or 'g_lex_utf8'\n",
    "# g_lex_utf is the lexeme part of the word, but as it is found in the manuscript. A word can have various spellings\n",
    "# lex is based on the lex feature. This means it has standard spelling and lexeme disambiguation.\n",
    "\n",
    "morpheme_dataset, _ = make_morpheme_dicts(n_clause_dict_augmented, 'lex', relevant_chars_utf8, alphabet_dict_heb)\n",
    "\n",
    "hebrew_bible_tokens = set()\n",
    "for token_string in morpheme_dataset.values():\n",
    "    token_set = set(token_string.split())\n",
    "    hebrew_bible_tokens.update(token_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf974688-252e-4b8f-8635-f4fa0aa96959",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'xbib_dict_len_{seq_length}.pkl', 'rb') as f:\n",
    "    xbib_morphemes_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9df2e32-5781-4ea9-8649-e47164292c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'syriac_dict_len_25_augm_10_s4.pkl', 'rb') as f:\n",
    "    syriac_morphemes_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1a2b4ff-0d45-4ece-8165-82880c9701dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178185"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpheme_dataset = {**morpheme_dataset, **xbib_morphemes_dict}\n",
    "len(morpheme_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "847d7c84-3f66-42a0-83cf-9fc3ab9faa17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191196"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morpheme_dataset = {**morpheme_dataset, **syriac_morphemes_dict}\n",
    "len(morpheme_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96a540be-94de-45a0-a73e-9093a5beb61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43039"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates in values (= texts)\n",
    "\n",
    "swapped_morpheme_dataset = {v:k for k, v in morpheme_dataset.items()}\n",
    "morpheme_dataset = {v:k for k, v in swapped_morpheme_dataset.items()}\n",
    "len(morpheme_dataset)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c831a0d-aa98-477b-af93-8f6f2ce5b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hebrew_bible_morphemes.txt', 'w', encoding='utf8') as f:\n",
    "    for heb_text in morpheme_dataset.values():\n",
    "        f.write(heb_text + '\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ac6c4a9-9384-4489-90f9-dc1ebedf93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '[UNK]'\n",
    "special_tokens = ['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]']\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
    "trainer = WordLevelTrainer(special_tokens = special_tokens)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single = '[CLS] $A [SEP]',\n",
    "    special_tokens = [('[CLS]', 1), ('[SEP]', 2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc7a05c1-0b20-4d59-a136-ebcb469ed22a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.train(['hebrew_bible_morphemes.txt'], trainer)\n",
    "\n",
    "tokenizer.save('morphemes_with_markers_tokenizer.json')\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file = 'morphemes_with_markers_tokenizer.json')\n",
    "\n",
    "tokenizer.add_special_tokens({'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70b0436b-0cf4-452b-8f69-3a7d39058cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12296"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11c251a7-7133-41c9-912a-a7e6fdbf98ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=12296, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1bef436-83ba-4bcf-891c-120bb4a5c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_df = pd.Series(morpheme_dataset).to_frame('text')\n",
    "bib_df_shuffled = bib_df.sample(frac=1, replace=False, ignore_index=True)\n",
    "bib_ds = Dataset.from_pandas(bib_df_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32c48937-8a8e-44be-8201-88439cd9f13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c14ae0cff54a8ab6e2d7733cae81a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43039 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(sentence):\n",
    "  return tokenizer(sentence['text'], max_length=128, truncation=True, padding=True)\n",
    "\n",
    "tokenized_data = bib_ds.map(tokenize, batched=True)\n",
    "tokenized_data.set_format(\"pt\", columns=[\"input_ids\", \"attention_mask\"], output_all_columns=True)\n",
    "tokenized_data = tokenized_data.train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf277d7f-34e6-4465-9ada-02d300b67f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=True, mlm_probability=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72a10540-485f-43b7-974b-dcc12f69a241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebaf148d-4aea-4bb3-82d4-7b3acfd70ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized because the shapes did not match:\n",
      "- bert.embeddings.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.embeddings.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.embeddings.position_embeddings.weight: found shape torch.Size([512, 768]) in the checkpoint and torch.Size([128, 256]) in the model instantiated\n",
      "- bert.embeddings.token_type_embeddings.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([2, 256]) in the model instantiated\n",
      "- bert.embeddings.word_embeddings.weight: found shape torch.Size([119547, 768]) in the checkpoint and torch.Size([12296, 256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.self.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.self.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.self.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.self.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.self.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.0.attention.self.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.0.intermediate.dense.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- bert.encoder.layer.0.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
      "- bert.encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.0.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.0.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.self.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.self.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.self.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.self.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.self.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.1.attention.self.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.1.intermediate.dense.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- bert.encoder.layer.1.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
      "- bert.encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.1.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.1.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.self.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.self.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.self.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.self.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.self.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.2.attention.self.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.2.intermediate.dense.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- bert.encoder.layer.2.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
      "- bert.encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.2.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.2.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.self.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.self.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.self.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.self.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.self.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.3.attention.self.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.3.intermediate.dense.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- bert.encoder.layer.3.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
      "- bert.encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.3.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.3.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.self.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.self.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.self.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.self.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.self.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.4.attention.self.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.4.intermediate.dense.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- bert.encoder.layer.4.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
      "- bert.encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.4.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.4.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.output.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.self.key.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.self.key.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.self.query.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.self.query.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.self.value.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.5.attention.self.value.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "- bert.encoder.layer.5.intermediate.dense.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([1024]) in the model instantiated\n",
      "- bert.encoder.layer.5.intermediate.dense.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([1024, 256]) in the model instantiated\n",
      "- bert.encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.5.output.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- bert.encoder.layer.5.output.dense.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([256, 1024]) in the model instantiated\n",
      "- cls.predictions.bias: found shape torch.Size([119547]) in the checkpoint and torch.Size([12296]) in the model instantiated\n",
      "- cls.predictions.transform.LayerNorm.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- cls.predictions.transform.LayerNorm.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- cls.predictions.transform.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([256]) in the model instantiated\n",
      "- cls.predictions.transform.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([256, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\n",
    "    'bert-base-multilingual-cased', \n",
    "    model_type='bert',\n",
    "    attention_probs_dropout_prob=.5, \n",
    "    hidden_dropout_prob=.5, \n",
    "    hidden_size=256,\n",
    "    intermediate_size=1024,\n",
    "    max_position_embeddings=128,\n",
    "    \n",
    "    num_attention_heads=num_attention_heads,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    vocab_size=len(tokenizer.vocab)\n",
    "    )\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased', config=config, ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "86a7a3d9-a2b0-42dd-a868-a5d901c07db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "BertForMaskedLM                                              --\n",
       "├─BertModel: 1-1                                             --\n",
       "│    └─BertEmbeddings: 2-1                                   --\n",
       "│    │    └─Embedding: 3-1                                   3,147,776\n",
       "│    │    └─Embedding: 3-2                                   32,768\n",
       "│    │    └─Embedding: 3-3                                   512\n",
       "│    │    └─LayerNorm: 3-4                                   512\n",
       "│    │    └─Dropout: 3-5                                     --\n",
       "│    └─BertEncoder: 2-2                                      --\n",
       "│    │    └─ModuleList: 3-6                                  4,738,560\n",
       "├─BertOnlyMLMHead: 1-2                                       --\n",
       "│    └─BertLMPredictionHead: 2-3                             --\n",
       "│    │    └─BertPredictionHeadTransform: 3-7                 66,304\n",
       "│    │    └─Linear: 3-8                                      3,160,072\n",
       "=====================================================================================\n",
       "Total params: 11,146,504\n",
       "Trainable params: 11,146,504\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43e6377a-1590-4304-9d30-05e6678547a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'morphs_marks_lex_6_layers_8_att_heads_5_seqlen_10_augm_with_xbib_syr_10_augm'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'morphs_marks_lex_{num_hidden_layers}_layers_{num_attention_heads}_att_heads_{seq_length}_seqlen_{augment_factor}_augm_with_xbib_syr_10_augm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26db386-78d1-4e11-869d-5b9d94eb51e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54545' max='344320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 54545/344320 28:05 < 2:29:16, 32.35 it/s, Epoch 12.67/80]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.958600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.659300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>5.534900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>5.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>5.482800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>5.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>5.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>5.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>5.315500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>5.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>5.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>5.126800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>5.130300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>5.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>5.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>5.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>5.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>5.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>5.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>5.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>5.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>5.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>5.067800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>5.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>5.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>5.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>5.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>5.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>5.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>5.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>5.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>5.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>5.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>5.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>4.996600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>5.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.990200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>4.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>4.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>4.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>4.983500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>4.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>4.997700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>4.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>4.958800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>4.938300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>4.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>4.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>4.950500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>4.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>4.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>4.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>4.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>4.949900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>4.900600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>4.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>4.885600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>4.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>4.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>4.900700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>4.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>4.917800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>4.895700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>4.868700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>4.881600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>4.916600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>4.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>4.870600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>4.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>4.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>4.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>4.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>4.853200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>4.866700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>4.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>4.846200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>4.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>4.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>4.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>4.800200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>4.833100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>4.827100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>4.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>4.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>4.783800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>4.813900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>4.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>4.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>4.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>4.784800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>4.776600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>4.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>4.746500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>4.745800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def randomize_model(model):\n",
    "    for module_ in model.named_modules(): \n",
    "        if isinstance(module_[1],(torch.nn.Linear, torch.nn.Embedding)):\n",
    "            module_[1].weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "        elif isinstance(module_[1], torch.nn.LayerNorm):\n",
    "            module_[1].bias.data.zero_()\n",
    "            module_[1].weight.data.fill_(1.0)\n",
    "        if isinstance(module_[1], torch.nn.Linear) and module_[1].bias is not None:\n",
    "            module_[1].bias.data.zero_()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = randomize_model(model)\n",
    "\n",
    "args = TrainingArguments(output_dir=f'morphs_marks_lex_{num_hidden_layers}_layers_{num_attention_heads}_att_heads_{seq_length}_seqlen_{augment_factor}_augm_with_xbib_syr_10_augm', \n",
    "                         save_strategy='epoch',\n",
    "                         learning_rate=0.0001,\n",
    "                         num_train_epochs=80,\n",
    "                         per_device_train_batch_size=8, \n",
    "                         per_device_eval_batch_size=8,\n",
    "                         seed=42,\n",
    "                        )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['test'],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "  )\n",
    "\n",
    "trainer.train()\n",
    "trainer.push_to_hub()\n",
    "\n",
    "\n",
    "#model_name = f'bert_mt_morphemes_with_markers_based_on_lexemes_{num_hidden_layers}_layers_{num_attention_heads}_att_heads'\n",
    "#trainer.save_model(os.path.join(MODEL_DIR, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e6310be6-14e7-4abe-8a6c-87efe056e9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'morphs_with_mark_based_on_lex_4_layers_8_att_heads_5_seqlen_10_augm_with_xbib_syr_no_disamb'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'morphs_with_mark_based_on_lex_{num_hidden_layers}_layers_{num_attention_heads}_att_heads_{seq_length}_seqlen_{augment_factor}_augm_with_xbib_syr_no_disamb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3dead29e-910f-481f-b9fe-5c2c0df4f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def get_top_k_predictions(text, k=5):\n",
    "    # Tokenize input with masking\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "\n",
    "    # Get top k tokens\n",
    "    top_k_tokens = torch.topk(mask_token_logits, k, dim=1).indices[0].tolist()\n",
    "    return tokenizer.convert_ids_to_tokens(top_k_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafe680-a894-44bb-9a34-931b316d7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = 'ב ראשית ברא אלה ֜ימ את ה שמי ֜מ ו את ה ארצ ו ה ארצ הי ֲתה תהו ו בהו ו חשכ על פנ ֜י תהומ ו רוח אלה ֜ימ ְמ רחפ ֜ת על פנ ֜י ה מי ֜מ'\n",
    "raw_text = 'ב ראשיתֶ בראַ אלהימֶ ֜ימ את ה שמימֶ ֜מ ו את ה ארצֶ ו ה ארצֶ היהַ ֲתה תהוֶ ו בהוֶ ו חשכֶ על פנהֶ ֜י תהומֶ ו רוחֶ אלהימֶ ֜ימ ְמ רחפַ ֶ֜ת על פנהֶ ֜י ה מימֶ ֜מ'\n",
    "\n",
    "text_split = raw_text.split()\n",
    "mask_char = '[MASK]'\n",
    "\n",
    "for i in range(5):\n",
    "    masked_text = ' '.join([text_split[i], text_split[i+1], mask_char, text_split[i+3], text_split[i+4], text_split[i+5], text_split[i+6]])\n",
    "    print(masked_text)\n",
    "    print('True value:', text_split[i+2])\n",
    "    top_predictions = get_top_k_predictions(masked_text, k=5)\n",
    "    print('Top prediction:', top_predictions[0])\n",
    "    print('Predictions', top_predictions)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b105b-269d-4463-9cb7-42347867db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'martijn75/bert_mt_morphemes_with_markers_based_on_lexemes_2_layers_4_att_heads'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(model_name, \n",
    "                                        return_dict_in_generate=True, \n",
    "                                        output_hidden_states=True).to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b54707-e302-45d2-abd6-0337bd63f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(morpheme_dataset, model, tokenizer):\n",
    "    hidden_states = {}\n",
    "    for key, texts_chunk in morpheme_dataset.items():\n",
    "        tokenized_inputs = tokenizer(texts_chunk, max_length=128, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        tokenized_inputs = {k:v.to(device) for k,v in tokenized_inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokenized_inputs)\n",
    "            last_hidden_states = outputs.hidden_states[-1].cpu().numpy()\n",
    "            hidden_states[key] = last_hidden_states\n",
    "\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa4da3-8391-4c7c-819f-c1662c5b520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_hidden_states(hidden_states):\n",
    "    hidden_states_mean = {}\n",
    "\n",
    "    for key, hs in hidden_states.items():\n",
    "        mean_state = np.mean(hs, 1)\n",
    "        mean_state = np.squeeze(mean_state)\n",
    "        hidden_states_mean[key] = mean_state\n",
    "        \n",
    "    return hidden_states_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b1f802-d5b6-4dd0-988b-89997f4dad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = get_hidden_states(morpheme_dataset, model, tokenizer)\n",
    "mean_hidden_states = calculate_mean_hidden_states(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8c5390-7fcb-4e55-bf25-65cc6b4bfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "emb1 = mean_hidden_states.get(('Genesis', 1, (427559, 427560, 427561, 427562)))\n",
    "emb2 =  mean_hidden_states.get(('Nehemiah', 10, (509499, 509500, 509501, 509502)))\n",
    "distance.cosine(emb1, emb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f16c27-5229-493a-b990-71ad77cdcdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "book = 'Jeremiah'\n",
    "\n",
    "jer_keys = [key for key in mean_hidden_states.keys() if key[0] == book]\n",
    "non_jer_keys = [key for key in mean_hidden_states.keys() if key[0] != book]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd7cf4-fbec-4c8d-850a-d04e6c3624b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "dim = (len(jer_keys), len(non_jer_keys))\n",
    "\n",
    "cosine_dists = np.zeros(dim)\n",
    "\n",
    "for jer_idx, jer_key in enumerate(jer_keys):\n",
    "    for non_jer_idx, non_jer_key in enumerate(non_jer_keys):\n",
    "        jer_emb = mean_hidden_states.get(jer_key)\n",
    "        non_jer_emb =  mean_hidden_states.get(non_jer_key)\n",
    "        dist = distance.cosine(jer_emb, non_jer_emb)\n",
    "        cosine_dists[jer_idx, non_jer_idx] = dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf046e-01f5-42fd-b00c-81921a79c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_jer_dict = collections.defaultdict(list)\n",
    "\n",
    "min_indices = np.argmin(cosine_dists, axis=1)\n",
    "len(min_indices)\n",
    "\n",
    "for jer_idx, non_jer_idx in enumerate(min_indices):\n",
    "    jer_key = jer_keys[jer_idx]\n",
    "    bo, ch, _ = jer_key\n",
    "    non_jer_key = non_jer_keys[non_jer_idx]\n",
    "    non_jer_bo, non_jer_ch, _ = non_jer_key\n",
    "    min_jer_dict[(bo, ch)].append((non_jer_bo, non_jer_ch))\n",
    "\n",
    "    print(cosine_dists[jer_idx, non_jer_idx])\n",
    "    print(non_jer_key, jer_key)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31112d94-7dc0-47fa-b2af-240aa0e2e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_jer_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
